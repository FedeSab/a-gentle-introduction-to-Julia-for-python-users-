{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Constants.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "full_pipe (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code is just an example, is not 100% optimized. \n",
    "Also, I'm not explaining why I use this preprocessing data because \n",
    "the aim of this project is testing Julia\n",
    "But I think its really interesting for starting with Julia\n",
    "\n",
    "# Example\n",
    "```full_pipe(\"application_train.csv\",\"application_test.csv\") #Or path to those files\n",
    "> \n",
    "```\n",
    "\n",
    "\n",
    "Structure:\n",
    "Imports and iniziate models.\n",
    "\n",
    "Read dataset, separate features and targets\n",
    "Drop columns\n",
    "NUMERICAL DATA:\n",
    "   - Use a simple \"for\" loop to impute missing values in numerical data using the median\n",
    "   - disallowmissing! to change type from Union{missing,Float64} to Float64\n",
    "   - coerce! to change the \"scientific notation\" of a column Count to a Continuous\n",
    "        Basically, not all types of operations are allowed to be perform in a Count type\n",
    "        (an ordinal sequence of number).\n",
    "   - Use machine(Standar) to make the normalization\n",
    "TEXT DATA:\n",
    "   - Similar to the NUM_DATA, we impute missing data with the most_frecuent values\n",
    "   - Then coerce! to change text data to a Multiclass\n",
    "   - Finally, we use OneHotEncoding\n",
    "BOOL_VALUES:\n",
    "   - Just using a \"for\" loop to impute missing data with the most_frecuent values\n",
    "\n",
    "Then we use the lightGBM to get the predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "using CSV #For reading CSV\n",
    "using DataFrames \n",
    "\n",
    "include(\"constants.jl\") #I import constants as modeule just to test\n",
    "using .Constants\n",
    "\n",
    "using StatsBase\n",
    "using MLJ\n",
    "using MLJ: fit! \n",
    "using MLJ: predict\n",
    "using MLJ: transform#Dataframes and MLJ both has fit! and transform methods\n",
    "\n",
    "using Missings #For dissallowing missing data\n",
    "\n",
    "LGBMClassifier = @load LGBMClassifier verbosity=0 #Here we load the models that we're gonna use\n",
    "OneHotEncoder = @load OneHotEncoder pkg=MLJModels verbosity=0\n",
    "\n",
    "function full_pipe(train,test)\n",
    "\n",
    "\n",
    "    lgb = LGBMClassifier() #initialised a model with default params\n",
    "\n",
    "    train_df = DataFrame(CSV.File(train))\n",
    "    test_df = DataFrame(CSV.File(test)) #Read .csv files\n",
    "\n",
    "    target_train = train_df[!,[\"TARGET\"]]\n",
    "    target_test = test_df[!,[\"TARGET\"]]\n",
    "    select!(train_df, Not([\"TARGET\"]))\n",
    "    select!(test_df, Not([\"TARGET\"]))\n",
    "    select!(train_df, Not(Constants.COLUMNS_TO_DROP))\n",
    "    select!(test_df, Not(Constants.COLUMNS_TO_DROP))\n",
    "\n",
    "    # Last time I check it, MLJ didn't implemented a SimpleImputer. But is really easy to code\n",
    "    num_features = Dict() #We want to store the median of numerical features\n",
    "    \n",
    "\n",
    "    for cols in Constants.COLUMNS_NUM\n",
    "\n",
    "        num_features[cols] = median(skipmissing(train_df[:,cols]))\n",
    "        \n",
    "        replace!(train_df[!,cols] ,\n",
    "            missing => num_features[cols])\n",
    "        \n",
    "        replace!(test_df[!,cols],  missing => num_features[cols])\n",
    "    end\n",
    "   \n",
    "    train_df[!,Constants.COLUMNS_NUM] = disallowmissing!(train_df[!,Constants.COLUMNS_NUM])\n",
    "    train_df[!,Constants.COLUMNS_NUM] = coerce!(train_df[!,Constants.COLUMNS_NUM], Count => Continuous)\n",
    "\n",
    "    test_df[!,Constants.COLUMNS_NUM] = disallowmissing!(test_df[!,Constants.COLUMNS_NUM])\n",
    "    test_df[!,Constants.COLUMNS_NUM] = coerce!(test_df[!,Constants.COLUMNS_NUM], Count => Continuous)\n",
    "\n",
    "    stand1 = machine(Standardizer(),train_df[!,Constants.COLUMNS_NUM])\n",
    "\n",
    "    fit!(stand1,verbosity=0)\n",
    "    \n",
    "    train_df[!,Constants.COLUMNS_NUM] = transform(stand1, train_df[!,Constants.COLUMNS_NUM])\n",
    "    test_df[!,Constants.COLUMNS_NUM] = transform(stand1, test_df[!,Constants.COLUMNS_NUM])\n",
    "\n",
    "\n",
    "    \n",
    "    str_features = Dict()\n",
    "\n",
    "    for cols in Constants.COLUMNS_STR\n",
    "\n",
    "        str_features[cols] = StatsBase.mode(skipmissing(train_df[:,cols]))\n",
    "        replace!(train_df[!,cols] ,\n",
    "            missing => str_features[cols])\n",
    "\n",
    "        replace!(test_df[!,cols],  missing => str_features[cols])\n",
    "    end\n",
    "\n",
    "    \n",
    "\n",
    "    train_df[!,Constants.COLUMNS_STR] = disallowmissing!(train_df[!,Constants.COLUMNS_STR])\n",
    "    train_df[!,Constants.COLUMNS_STR] = coerce!(train_df[!,Constants.COLUMNS_STR], Textual => Multiclass)\n",
    "\n",
    "    test_df[!,Constants.COLUMNS_STR] = disallowmissing!(test_df[!,Constants.COLUMNS_STR])\n",
    "    test_df[!,Constants.COLUMNS_STR] = coerce!(test_df[!,Constants.COLUMNS_STR], Textual => Multiclass)\n",
    "\n",
    "    hot = machine(OneHotEncoder(drop_last=true), train_df[!,Constants.COLUMNS_STR])\n",
    "    \n",
    "    fit!(hot,verbosity=0) \n",
    "    train_df = hcat(select(train_df, Not(Constants.COLUMNS_STR)),\n",
    "                    transform(hot, train_df[!,Constants.COLUMNS_STR]))\n",
    "\n",
    "    test_df = hcat(select(test_df, Not(Constants.COLUMNS_STR)),\n",
    "                    transform(hot, test_df[!,Constants.COLUMNS_STR]))\n",
    "\n",
    "    bool_features = Dict()\n",
    "\n",
    "    for cols in Constants.COLUMNS_BOOL\n",
    "        \n",
    "        bool_features[cols] = StatsBase.mode(skipmissing(train_df[:,cols]))\n",
    "        replace!(train_df[!,cols] ,\n",
    "            missing => bool_features[cols])\n",
    "\n",
    "        replace!(test_df[!,cols], missing => bool_features[cols])\n",
    "    end\n",
    "\n",
    "    target_train = coerce!(target_train,  Count => Multiclass )\n",
    "    target_test = coerce!(target_test,  Count => Multiclass ) \n",
    "\n",
    "    lgbm = machine(lgb, train_df, target_train[!,1])\n",
    "    \n",
    "    fit!(lgbm)\n",
    "    \n",
    "    train_predict = predict(lgbm, train_df)\n",
    "    test_predict = predict(lgbm, test_df)\n",
    "\n",
    "    train_auc = auc(train_predict, target_train[!,1])\n",
    "    test_auc = auc(test_predict, target_test[!,1])\n",
    "    println(train_auc )\n",
    "    println(test_auc )\n",
    "    return train_auc, test_auc\n",
    "    \n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc LightGBM.LGBMClassifier` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}, AbstractVector{Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Union{Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}, Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:230\n",
      "┌ Info: Training machine(LGBMClassifier(boosting = gbdt, …), …).\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6227\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score -0.084162\n",
      "[LightGBM] [Info] Start training from score -2.516800\n",
      "0.735051420150909\n",
      "0.7321127469927498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc LightGBM.LGBMClassifier` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}, AbstractVector{Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Union{Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}, Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:230\n",
      "┌ Info: Training machine(LGBMClassifier(boosting = gbdt, …), …).\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6227\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score -0.084162\n",
      "[LightGBM] [Info] Start training from score -2.516800\n",
      "0.735051420150909\n",
      "0.7321127469927498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc LightGBM.LGBMClassifier` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}, AbstractVector{Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Union{Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}, Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:230\n",
      "┌ Info: Training machine(LGBMClassifier(boosting = gbdt, …), …).\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6227\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score -0.084162\n",
      "[LightGBM] [Info] Start training from score -2.516800\n",
      "0.735051420150909\n",
      "0.7321127469927498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc LightGBM.LGBMClassifier` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}, AbstractVector{Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Union{Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}}, Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:230\n",
      "┌ Info: Training machine(LGBMClassifier(boosting = gbdt, …), …).\n",
      "└ @ MLJBase C:\\Users\\User\\.julia\\packages\\MLJBase\\VTQPI\\src\\machines.jl:492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6227\n",
      "[LightGBM] [Info] Number of data points in the train set: 215257, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score -0.084162\n",
      "[LightGBM] [Info] Start training from score -2.516800\n",
      "0.735051420150909\n",
      "0.7321127469927498\n",
      "  15.232 s (37537746 allocations: 4.66 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.735051420150909, 0.7321127469927498)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "@btime full_pipe(\"application_train.csv\",\"application_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
